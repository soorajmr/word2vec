{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> <span style=\"color:blue\"> Word2Vec demo on Shakespeare's Works </span> </h1>\n",
    "\n",
    "<h4 align='center'>[To accompany CSCI E-81 paper presentation by Anand Bonthu and Sooraj Raveendran]</h4>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords  # Import the stop word list\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_full_text_links(soup, col, base_url, genre):\n",
    "    links = []\n",
    "    for row in soup.find_all(\"table\")[1].find_all(\"tr\"):\n",
    "        anchors = row.find_all(\"td\")[col].find_all(\"a\")\n",
    "        for a in anchors:\n",
    "            link = a.get(\"href\")\n",
    "            title = a.get_text()\n",
    "            # remove unwanted newline characters\n",
    "            title = re.sub('^\\n', '', title)\n",
    "            title = re.sub('\\n', ' ', title)\n",
    "            # replace the links with the full-text page links\n",
    "            link = re.sub(\"index.html\", \"full.html\", link)\n",
    "            links.append(dict(title=title, link = base_url + '/' + link, genre=genre))\n",
    "            \n",
    "    return links\n",
    "\n",
    "def get_sonnet_links(soup, col, base_url):\n",
    "    links = []\n",
    "    row = soup.find_all(\"table\")[1].find_all(\"tr\")[1]\n",
    "    anchor = row.find_all(\"td\")[col].find_all(\"a\")[0]\n",
    "    sonnets_page_url = base_url + '/' + anchor.get(\"href\")\n",
    "    sonnets_page = requests.get(sonnets_page_url)\n",
    "    soup_sonnet = BeautifulSoup(sonnets_page.text, \"html.parser\")\n",
    "    count = 1\n",
    "    for row in soup_sonnet.find_all(\"dt\"):\n",
    "        sonnet_url = base_url + '/' + row.find(\"a\").get(\"href\")\n",
    "        links.append(dict(title = \"sonnet-\" + str(count), link = sonnet_url, genre=\"sonnet\"))\n",
    "        count += 1\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_other_poetry_links(soup, col, base_url):\n",
    "    links = []\n",
    "    for row in soup.find_all(\"table\")[1].find_all(\"tr\"):\n",
    "        anchors = row.find_all(\"td\")[col].find_all(\"a\")\n",
    "        for a in anchors:\n",
    "            link = a.get(\"href\")\n",
    "            title = a.get_text()\n",
    "            # remove unwanted newline characters\n",
    "            title = re.sub('^\\n', '', title)\n",
    "            title = re.sub('\\n', ' ', title)\n",
    "            # replace the links with the full-text page links\n",
    "            #link = re.sub(\"index.html\", \"full.html\", link)\n",
    "            if(title != 'The Sonnets'):\n",
    "                links.append(dict(title = title, link = base_url + '/' + link, genre=\"other_poetry\"))\n",
    "            \n",
    "    return links\n",
    "\n",
    "def fetch_text(works):\n",
    "    works_downloaded = []\n",
    "    for work in works:\n",
    "        time.sleep(1) # do not swamp the server\n",
    "        work_page = requests.get(work.get('link'))\n",
    "        works_downloaded.append(dict(title=work.get('title'), genre=work.get('genre'), raw_html=work_page.text))\n",
    "        \n",
    "    return works_downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = \"http://shakespeare.mit.edu\"\n",
    "main_page = requests.get(base_url)\n",
    "soup = BeautifulSoup(main_page.text, \"html.parser\")\n",
    "\n",
    "## Parse the table in the main page to get the links to the page for each work\n",
    "\n",
    "## Column-1: Comedy\n",
    "comedy = get_full_text_links(soup, 0, base_url, \"comedy\")\n",
    "\n",
    "## Column-2: History\n",
    "history = get_full_text_links(soup, 1, base_url, \"history\")\n",
    "\n",
    "## Column-3: Tragedy\n",
    "tragedy = get_full_text_links(soup, 2, base_url, \"tragedy\")\n",
    "\n",
    "## Column-4: Poetry - sonnets\n",
    "sonnets = get_sonnet_links(soup, 3, base_url)    \n",
    "\n",
    "## Column-4: Poetry - other poems\n",
    "other_poems = get_other_poetry_links(soup, 3, base_url)\n",
    "\n",
    "## Download all text from all the urls collected so far\n",
    "complete_works = fetch_text(comedy + history + tragedy + sonnets + other_poems)\n",
    "\n",
    "## Save the downloaded data\n",
    "with open(\"completeworks.json\", \"w\") as fd:\n",
    "    json.dump(complete_works, fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parse Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the previously saved html pages of all works\n",
    "with open(\"completeworks.json\", \"r\") as fd:\n",
    "    all_works_html = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Remove words irrelevant for our purpose\n",
    "'''\n",
    "def remove_stopwords(wordlist):\n",
    "    shakespeare_stopwords = ['thou', 'thy', 'thine', 'thee', 'd', 'ye', 'doth', 'dost', 'hath', 'nor', 'th', 'shalt', 'enter']\n",
    "    new_list = [w for w in wordlist if not w in (shakespeare_stopwords + stopwords.words(\"english\"))]\n",
    "    return new_list\n",
    "\n",
    "'''\n",
    "Tokenize and stem\n",
    "'''\n",
    "def text_to_wordlist(text):\n",
    "    text_cleaned = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = text_cleaned.lower().split() ## TODO - use nltk tokenizer\n",
    "    #stemmer = SnowballStemmer(\"english\")\n",
    "    #stemmed_words = [stemmer.stem(w) for w in words]\n",
    "    cleaned_wordlist = words #remove_stopwords(stemmed_words)\n",
    "    return cleaned_wordlist\n",
    "\n",
    "'''\n",
    "Extract all text from the html page\n",
    "'''\n",
    "def parse_complete_text(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    plain_text = \"\"\n",
    "    for block in soup.find_all(\"blockquote\"):\n",
    "        plain_text += block.get_text()\n",
    "\n",
    "    return text_to_wordlist(plain_text)\n",
    "    \n",
    "'''\n",
    "Special handling for parsing the Funeral Elegy page\n",
    "'''\n",
    "def parse_complete_text_elegy(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    \n",
    "    ## Possibly some bug in BS - for some reason all text comes in the first 'tr' node.\n",
    "    ## If you loop through the rows, it gets into infinite recursion.\n",
    "    \n",
    "    #for row in soup.find_all('table')[0].find_all('tr'):\n",
    "    #    for cell in row.find_all('td'):\n",
    "    #        if(cell != None):\n",
    "    #            plain_text += cell.get_text()\n",
    "    plain_text = soup.find_all('table')[0].find_all('tr')[0].get_text()\n",
    "    return text_to_wordlist(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_all_works_dataframe(all_works_html):\n",
    "    all_works_text = []    \n",
    "\n",
    "    for work in all_works_html:\n",
    "        if(work.get('title') == 'Funeral Elegy by W.S.'):\n",
    "            # Special case\n",
    "            d = dict(title = work.get('title'), \n",
    "                     genre = work.get('genre'), \n",
    "                     text = parse_complete_text_elegy(work.get('raw_html')))\n",
    "        else:\n",
    "            d = dict(title = work.get('title'), \n",
    "                     genre = work.get('genre'), \n",
    "                     text = parse_complete_text(work.get('raw_html')))\n",
    "\n",
    "        all_works_text.append(d)\n",
    "\n",
    "    return pd.DataFrame(all_works_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We create three Pandas DataFrames that hold the cleaned up corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following can take a few minutes\n",
    "\n",
    "all_works_df = create_all_works_dataframe(all_works_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comedy</td>\n",
       "      <td>[enter, bertram, the, countess, of, rousillon,...</td>\n",
       "      <td>All's Well That Ends Well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comedy</td>\n",
       "      <td>[enter, orlando, and, adam, as, i, remember, a...</td>\n",
       "      <td>As You Like It</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comedy</td>\n",
       "      <td>[enter, duke, solinus, aegeon, gaoler, officer...</td>\n",
       "      <td>The Comedy of Errors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comedy</td>\n",
       "      <td>[enter, two, gentlemen, you, do, not, meet, a,...</td>\n",
       "      <td>Cymbeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comedy</td>\n",
       "      <td>[enter, ferdinand, king, of, navarre, biron, l...</td>\n",
       "      <td>Love's Labours Lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    genre                                               text  \\\n",
       "0  comedy  [enter, bertram, the, countess, of, rousillon,...   \n",
       "1  comedy  [enter, orlando, and, adam, as, i, remember, a...   \n",
       "2  comedy  [enter, duke, solinus, aegeon, gaoler, officer...   \n",
       "3  comedy  [enter, two, gentlemen, you, do, not, meet, a,...   \n",
       "4  comedy  [enter, ferdinand, king, of, navarre, biron, l...   \n",
       "\n",
       "                       title  \n",
       "0  All's Well That Ends Well  \n",
       "1             As You Like It  \n",
       "2       The Comedy of Errors  \n",
       "3                  Cymbeline  \n",
       "4        Love's Labours Lost  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_works_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. OK, Now We're Ready to Run Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# train word2vec on the all Shakespeare words\n",
    "model = gensim.models.Word2Vec(list(all_works_df.text), min_count=10, size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odd one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'french'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"england france french spain denmark\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'polonius', 0.6881860494613647),\n",
       " (u'publius', 0.6694139838218689),\n",
       " (u'marcellus', 0.6688525080680847),\n",
       " (u'brutus', 0.6660898923873901),\n",
       " (u'portia', 0.6638930439949036)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['hamlet'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'husband', 0.7530255913734436),\n",
       " (u'friend', 0.7111741900444031),\n",
       " (u'father', 0.7002392411231995),\n",
       " (u'grandam', 0.6747819185256958),\n",
       " (u'wife', 0.6672350764274597),\n",
       " (u'son', 0.6559919714927673),\n",
       " (u'soul', 0.6557852625846863),\n",
       " (u'brother', 0.6099331378936768),\n",
       " (u'curse', 0.608603298664093),\n",
       " (u'traitor', 0.6082962155342102)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['mother'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mother:Wife :: Father:?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'grandam', 0.6607803106307983),\n",
       " (u'husband', 0.645194947719574),\n",
       " (u'friend', 0.6131525039672852),\n",
       " (u'slanderous', 0.5877306461334229),\n",
       " (u'deserved', 0.572910726070404),\n",
       " (u'praising', 0.5640237927436829),\n",
       " (u'child', 0.5584622025489807),\n",
       " (u'dearly', 0.5559671521186829),\n",
       " (u'sake', 0.5434399843215942),\n",
       " (u'fault', 0.5379261374473572)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['mother', 'wife'], negative=['father'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And this won't be complete without this example!! :-)\n",
    "\n",
    "### King + Man - Queen = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.6004334688186646),\n",
       " (u'crab', 0.567439079284668),\n",
       " (u'sweeter', 0.5656701922416687),\n",
       " (u'hour', 0.542351245880127),\n",
       " (u'time', 0.5357239842414856)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['king', 'man'], negative=['queen'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
